{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c5b707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_preview = pd.read_csv(news_csv, nrows=5)\n",
    "# Display the first row (article) of the dataframe\n",
    "# print(df_preview.iloc[0, 7]) # text is in the 8th column use .iloc[row, column]\n",
    "\n",
    "# print the last row (article) of the dataframe\n",
    "# print(df_preview.iloc[-1, 7]) # text is in the 8th column use .iloc[row, column]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a6a87c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(news_csv, \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     33\u001b[39m     reader = csv.DictReader(f)\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m        \u001b[49m\u001b[43msym\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mStock_symbol\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGOOG\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msym\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGOOGL\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msym\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python312\\Lib\\csv.py:116\u001b[39m, in \u001b[36mDictReader.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.line_num == \u001b[32m0\u001b[39m:\n\u001b[32m    114\u001b[39m     \u001b[38;5;66;03m# Used only for its side effect.\u001b[39;00m\n\u001b[32m    115\u001b[39m     \u001b[38;5;28mself\u001b[39m.fieldnames\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m row = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[38;5;28mself\u001b[39m.line_num = \u001b[38;5;28mself\u001b[39m.reader.line_num\n\u001b[32m    119\u001b[39m \u001b[38;5;66;03m# unlike the basic reader, we prefer not to return blanks,\u001b[39;00m\n\u001b[32m    120\u001b[39m \u001b[38;5;66;03m# because we will typically wind up with a dict full of None\u001b[39;00m\n\u001b[32m    121\u001b[39m \u001b[38;5;66;03m# values\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen codecs>:319\u001b[39m, in \u001b[36mdecode\u001b[39m\u001b[34m(self, input, final)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "DATA_DIR = \"../data/raw/FNSPID\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "# ----------------------------\n",
    "# Step 1: Download Stock History Zip (already done)\n",
    "# ----------------------------\n",
    "# stock_url = \"https://huggingface.co/datasets/Zihan1004/FNSPID/resolve/main/Stock_price/full_history.zip\"\n",
    "# stock_zip = os.path.join(DATA_DIR, \"full_history.zip\")\n",
    "# download_file(stock_url, stock_zip)\n",
    "\n",
    "needed_tickers = [\"AAPL.csv\", \"MSFT.csv\", \"GOOG.csv\", \"AMZN.csv\"]\n",
    "# with zipfile.ZipFile(stock_zip, \"r\") as z:\n",
    "#     for file in needed_tickers:\n",
    "#         if file in z.namelist():\n",
    "#             z.extract(file, DATA_DIR)\n",
    "\n",
    "# ----------------------------\n",
    "# Step 2: Download News File (already done — you have 5.97GB CSV)\n",
    "# ----------------------------\n",
    "# news_url = \"https://huggingface.co/datasets/Zihan1004/FNSPID/resolve/main/Stock_news/nasdaq_exteral_data.csv\"\n",
    "news_csv = os.path.join(DATA_DIR, \"nasdaq_exteral_data.csv\")\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Step 3: Filter News by Tickers + Date (STREAMING)\n",
    "# ----------------------------\n",
    "output_file = os.path.join(DATA_DIR, \"news_filtered.csv\")\n",
    "\n",
    "tickers = {\"AAPL\", \"MSFT\", \"GOOG\", \"AMZN\"}\n",
    "start_date = pd.to_datetime(\"2020-09-16\")\n",
    "end_date = pd.to_datetime(\"2025-09-15\")\n",
    "\n",
    "with open(news_csv, \"r\", encoding=\"utf-8\") as f_in, \\\n",
    "     open(output_file, \"w\", encoding=\"utf-8\", newline=\"\") as f_out:\n",
    "\n",
    "    reader = csv.DictReader(f_in)\n",
    "    writer = csv.DictWriter(f_out, fieldnames=reader.fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    kept = 0\n",
    "    processed = 0\n",
    "    for row in reader:\n",
    "        processed += 1\n",
    "        if row[\"Stock_symbol\"] not in tickers:\n",
    "            continue\n",
    "        try:\n",
    "            d = pd.to_datetime(row[\"Date\"]).tz_localize(None)\n",
    "        except Exception:\n",
    "            continue\n",
    "        if start_date <= d <= end_date:\n",
    "            writer.writerow(row)\n",
    "            kept += 1\n",
    "\n",
    "        # log every 1M rows so you see progress\n",
    "        if processed % 1_000_000 == 0:\n",
    "            print(f\"Processed {processed:,} rows… kept {kept:,}\")\n",
    "\n",
    "print(f\"✅ Done! Filtered rows saved to {output_file}\")\n",
    "print(f\"📊 Total rows kept: {kept}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84125929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found MSFT symbols in file: set()\n"
     ]
    }
   ],
   "source": [
    "# # see if Google symbol is present in the original file\n",
    "# import csv\n",
    "\n",
    "# news_csv = \"../data/raw/FNSPID/nasdaq_exteral_data.csv\"\n",
    "\n",
    "# found = set()\n",
    "# with open(news_csv, \"r\", encoding=\"utf-8\") as f:\n",
    "#     reader = csv.DictReader(f)\n",
    "#     for i, row in enumerate(reader):\n",
    "#         sym = row[\"Stock_symbol\"]\n",
    "#         if \"MSFT\" in sym or \"MSFT\" in sym:\n",
    "#             found.add(sym)\n",
    "#         if i > 2_000_000:  # stop after 2M rows for speed\n",
    "#             break\n",
    "\n",
    "# print(\"✅ Found MSFT symbols in file:\", found)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc93b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symbols found for MSFT/GOOG: set()\n"
     ]
    }
   ],
   "source": [
    "# # see how GOOGL, and MSFT are represented in the original file\n",
    "# import csv\n",
    "\n",
    "# symbols = set()\n",
    "# with open(\"../data/raw/FNSPID/nasdaq_exteral_data.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     reader = csv.DictReader(f)\n",
    "#     for i, row in enumerate(reader):\n",
    "#         sym = row[\"Stock_symbol\"]\n",
    "#         if \"MSFT\" in sym or \"GOOG\" in sym:\n",
    "#             symbols.add(sym)\n",
    "#         if i > 2_000_000:  # stop early for speed\n",
    "#             break\n",
    "\n",
    "# print(\"Symbols found for MSFT/GOOG:\", symbols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ca3123ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.nasdaq.com/articles/brokers-suggest-investing-in-apple-aapl%3A-read-this-before-placing-a-bet\n",
      "13647\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"C:/Users/User/Desktop/Data Science/Fall 2025/Generative AI/Project/genai-financial-doc-analysis/data/raw/FNSPID/news_filtered.csv\")\n",
    "\n",
    "print(df.iloc[1, 4]) # text is in the 8th column use .iloc[row, column]\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea59082",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Option 2:\n",
    "# Select keywords, ticker names, sectors, and download only those rows fro the dataset which should be a lot faster. Download all values of a row if the criteria is met.\n",
    "\n",
    "# Regex friendly eyword list for filtering news\n",
    "\n",
    "# Ticker/sector alignment so each article gets linked to a stock ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c935879d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Filtering news file for tickers and keywords...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_2580\\2810447847.py:79: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (chunk[\"Article\"].str.contains(pattern, na=False)) |\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_2580\\2810447847.py:80: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (chunk[\"Article_title\"].str.contains(pattern, na=False))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Added 42471 rows (total: 42471)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_2580\\2810447847.py:79: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (chunk[\"Article\"].str.contains(pattern, na=False)) |\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_2580\\2810447847.py:80: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (chunk[\"Article_title\"].str.contains(pattern, na=False))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Added 34545 rows (total: 77016)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_2580\\2810447847.py:79: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (chunk[\"Article\"].str.contains(pattern, na=False)) |\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_2580\\2810447847.py:80: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (chunk[\"Article_title\"].str.contains(pattern, na=False))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Added 38147 rows (total: 115163)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_2580\\2810447847.py:79: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (chunk[\"Article\"].str.contains(pattern, na=False)) |\n"
     ]
    }
   ],
   "source": [
    "# # ----------------------------\n",
    "# # 01 - Data Collection (News Section)\n",
    "# # ----------------------------\n",
    "\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# import re\n",
    "# import requests\n",
    "\n",
    "# # ----------------------------\n",
    "# # Setup paths\n",
    "# # ----------------------------\n",
    "# DATA_DIR = \"../data/raw/FNSPID\"\n",
    "# os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "# news_url = \"https://huggingface.co/datasets/Zihan1004/FNSPID/resolve/main/Stock_news/nasdaq_exteral_data.csv\"\n",
    "# news_csv = os.path.join(DATA_DIR, \"nasdaq_exteral_data_selective.csv\")\n",
    "# filtered_news_path = os.path.join(DATA_DIR, \"news_filtered.csv\")\n",
    "\n",
    "# # ----------------------------\n",
    "# # Download News File (if not present)\n",
    "# # ----------------------------\n",
    "# # def download_file(url, out_path):\n",
    "# #     if os.path.exists(out_path):\n",
    "# #         print(f\"✅ Already downloaded: {out_path}\")\n",
    "# #         return\n",
    "# #     print(f\"⬇️ Downloading {url} ... (this is a large file, ~8GB)\")\n",
    "# #     with requests.get(url, stream=True) as r:\n",
    "# #         r.raise_for_status()\n",
    "# #         with open(out_path, \"wb\") as f:\n",
    "# #             for chunk in r.iter_content(chunk_size=8192):\n",
    "# #                 if chunk:\n",
    "# #                     f.write(chunk)\n",
    "# #     print(f\"✅ Saved to {out_path}\")\n",
    "\n",
    "# # download_file(news_url, news_csv)\n",
    "\n",
    "# # ----------------------------\n",
    "# # Define Filtering Keywords\n",
    "# # ----------------------------\n",
    "# tickers = [\"AAPL\",\"MSFT\",\"GOOG\",\"AMZN\"]\n",
    "\n",
    "# keywords = [\n",
    "#     r\"\\binflation\\b\", r\"\\bdeflation\\b\", \n",
    "#     r\"\\binterest rate(s)?\\b\", r\"\\brate (hike|cut)\\b\", \n",
    "#     r\"\\bfederal reserve\\b\", r\"\\bfed\\b\", \n",
    "#     r\"\\bquantitative easing\\b\", r\"\\btightening\\b\", \n",
    "#     r\"\\brecession\\b\", r\"\\bgdp (growth|contraction)\\b\", \n",
    "#     r\"\\bunemployment\\b\", r\"\\bjobless claims?\\b\", \n",
    "#     r\"\\belection\\b\", r\"\\bgovernment shutdown\\b\", \n",
    "#     r\"\\btrade war\\b\", r\"\\btariffs?\\b\", \n",
    "#     r\"\\bsanctions?\\b\", r\"\\bgeopolitical tension(s)?\\b\", \n",
    "#     r\"\\boil (embargo|shock)\\b\", \n",
    "#     r\"\\bvolatility spike\\b\", r\"\\bvix\\b\", \n",
    "#     r\"\\bliquidity crisis\\b\", r\"\\bcredit downgrade\\b\", \n",
    "#     r\"\\bbank failure\\b\", r\"\\bbailout\\b\", \n",
    "#     r\"\\bdefault(s)?\\b\", r\"\\bdebt ceiling\\b\", \n",
    "#     r\"\\bearnings (miss|beat)\\b\", r\"\\bguidance cut\\b\", \n",
    "#     r\"\\bipo (surge|slump)\\b\", r\"\\b(merger|acquisition)\\b\", \n",
    "#     r\"\\bantitrust investigation\\b\", \n",
    "#     r\"\\bpandemic\\b\", r\"\\bcovid\\b\", \n",
    "#     r\"\\bsupply chain disruption\\b\"\n",
    "# ]\n",
    "\n",
    "# pattern = re.compile(\"|\".join(keywords), flags=re.IGNORECASE)\n",
    "\n",
    "# # ----------------------------\n",
    "# # Filter News in Chunks (Updated for FNSPID schema)\n",
    "# # ----------------------------\n",
    "# print(\"⏳ Filtering news file for tickers and keywords...\")\n",
    "\n",
    "# chunksize = 100_000\n",
    "# first = True\n",
    "# rows_written = 0\n",
    "\n",
    "# for chunk in pd.read_csv(news_csv, chunksize=chunksize, low_memory=False):\n",
    "#     mask = (\n",
    "#         (chunk[\"Stock_symbol\"].isin(tickers)) |\n",
    "#         (chunk[\"Article\"].str.contains(pattern, na=False)) |\n",
    "#         (chunk[\"Article_title\"].str.contains(pattern, na=False))\n",
    "#     )\n",
    "#     sub = chunk[mask]\n",
    "\n",
    "#     if not sub.empty:\n",
    "#         sub.to_csv(filtered_news_path, mode=\"a\", index=False, header=first)\n",
    "#         rows_written += len(sub)\n",
    "#         first = False\n",
    "#         print(f\"✅ Added {len(sub)} rows (total: {rows_written})\")\n",
    "\n",
    "# print(\"🎉 Finished filtering news!\")\n",
    "# print(\"✅ Saved filtered news to:\", filtered_news_path)\n",
    "\n",
    "# # ----------------------------\n",
    "# # Quick Sanity Check\n",
    "# # ----------------------------\n",
    "# news_filtered = pd.read_csv(filtered_news_path, nrows=5)\n",
    "# print(news_filtered[[\"Date\",\"Stock_symbol\",\"Article_title\"]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9731e0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'Date', 'Article_title', 'Stock_symbol', 'Url',\n",
      "       'Publisher', 'Author', 'Article', 'Lsa_summary', 'Luhn_summary',\n",
      "       'Textrank_summary', 'Lexrank_summary'],\n",
      "      dtype='object')\n",
      "   Unnamed: 0                     Date  \\\n",
      "0         0.0  2023-12-16 23:00:00 UTC   \n",
      "1         1.0  2023-12-12 00:00:00 UTC   \n",
      "2         2.0  2023-12-12 00:00:00 UTC   \n",
      "3         3.0  2023-12-07 00:00:00 UTC   \n",
      "4         4.0  2023-12-07 00:00:00 UTC   \n",
      "\n",
      "                                       Article_title Stock_symbol  \\\n",
      "0  Interesting A Put And Call Options For August ...            A   \n",
      "1  Wolfe Research Initiates Coverage of Agilent T...            A   \n",
      "2  Agilent Technologies Reaches Analyst Target Price            A   \n",
      "3  Agilent (A) Enhances BioTek Cytation C10 With ...            A   \n",
      "4  Pre-Market Most Active for Dec 7, 2023 : SQQQ,...            A   \n",
      "\n",
      "                                                 Url  Publisher  Author  \\\n",
      "0  https://www.nasdaq.com/articles/interesting-a-...        NaN     NaN   \n",
      "1  https://www.nasdaq.com/articles/wolfe-research...        NaN     NaN   \n",
      "2  https://www.nasdaq.com/articles/agilent-techno...        NaN     NaN   \n",
      "3  https://www.nasdaq.com/articles/agilent-a-enha...        NaN     NaN   \n",
      "4  https://www.nasdaq.com/articles/pre-market-mos...        NaN     NaN   \n",
      "\n",
      "                                             Article  \\\n",
      "0  Investors in Agilent Technologies, Inc. (Symbo...   \n",
      "1  Fintel reports that on December 13, 2023, Wolf...   \n",
      "2  In recent trading, shares of Agilent Technolog...   \n",
      "3  Agilent Technologies A is enhancing its BioTek...   \n",
      "4  The NASDAQ 100 Pre-Market Indicator is up 70.2...   \n",
      "\n",
      "                                         Lsa_summary  \\\n",
      "0  Because the $125.00 strike represents an appro...   \n",
      "1  Fintel reports that on December 13, 2023, Wolf...   \n",
      "2  In recent trading, shares of Agilent Technolog...   \n",
      "3  Per a Grand View Research report, the global m...   \n",
      "4  ProShares UltraPro Short QQQ (SQQQ) is -0.15 a...   \n",
      "\n",
      "                                        Luhn_summary  \\\n",
      "0  The current analytical data (including greeks ...   \n",
      "1  T. Rowe Price Investment Management holds 10,1...   \n",
      "2  In recent trading, shares of Agilent Technolog...   \n",
      "3  Notably, Agilent enhanced the BioTek Cytation ...   \n",
      "4  As reported by Zacks, the current mean recomme...   \n",
      "\n",
      "                                    Textrank_summary  \\\n",
      "0  Below is a chart showing the trailing twelve m...   \n",
      "1  Agilent Technologies Declares $0.24 Dividend O...   \n",
      "2  When a stock reaches the target an analyst has...   \n",
      "3  Agilent Technologies, Inc. Price and Consensus...   \n",
      "4  The total Pre-Market volume is currently 39,23...   \n",
      "\n",
      "                                     Lexrank_summary  \n",
      "0  At Stock Options Channel, our YieldBoost formu...  \n",
      "1  The projected annual revenue for Agilent Techn...  \n",
      "2  When a stock reaches the target an analyst has...  \n",
      "3  Notably, Agilent enhanced the BioTek Cytation ...  \n",
      "4  The NASDAQ 100 Pre-Market Indicator is up 70.2...  \n"
     ]
    }
   ],
   "source": [
    "# news_csv = r\"C:\\Users\\User\\Desktop\\Data Science\\Fall 2025\\Generative AI\\Project\\genai-financial-doc-analysis\\data\\raw\\FNSPID\\nasdaq_exteral_data_selective.csv\"\n",
    "\n",
    "# df_sample = pd.read_csv(news_csv, nrows=5)\n",
    "# print(df_sample.columns)\n",
    "# print(df_sample.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
