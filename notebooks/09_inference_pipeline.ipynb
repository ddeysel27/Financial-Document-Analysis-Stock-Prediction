{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2cc64a3",
   "metadata": {},
   "source": [
    "09_inference_pipeline.ipynb notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4652cce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_df before FE: (502, 10)\n",
      "Nulls in sentiment: 502\n",
      "          Date        Open        High         Low       Close    Volume  \\\n",
      "482 2025-08-18  230.229996  231.910004  228.330002  231.490005  25248900   \n",
      "483 2025-08-19  230.089996  230.529999  227.119995  228.009995  29891000   \n",
      "484 2025-08-20  227.119995  227.270004  220.919998  223.809998  36604300   \n",
      "485 2025-08-21  222.649994  222.779999  220.500000  221.949997  32140500   \n",
      "486 2025-08-22  222.789993  229.139999  220.820007  228.839996  37315300   \n",
      "487 2025-08-25  227.350006  229.600006  227.309998  227.940002  22633700   \n",
      "488 2025-08-26  227.110001  229.000000  226.020004  228.710007  26105400   \n",
      "489 2025-08-27  228.570007  229.869995  227.809998  229.119995  21254500   \n",
      "490 2025-08-28  229.009995  232.710007  228.020004  231.600006  33679600   \n",
      "491 2025-08-29  231.320007  231.809998  228.160004  229.000000  26199200   \n",
      "492 2025-09-02  223.520004  226.169998  221.830002  225.339996  38843900   \n",
      "493 2025-09-03  225.210007  227.169998  224.360001  225.990005  29223100   \n",
      "494 2025-09-04  231.190002  235.770004  230.779999  235.679993  59391800   \n",
      "495 2025-09-05  235.190002  236.000000  231.929993  232.330002  36721800   \n",
      "496 2025-09-08  234.940002  237.600006  233.750000  235.839996  33947100   \n",
      "497 2025-09-09  236.360001  238.850006  235.080002  238.240005  27033800   \n",
      "498 2025-09-10  237.520004  237.679993  229.100006  230.330002  60907700   \n",
      "499 2025-09-11  231.490005  231.529999  229.339996  229.949997  37485600   \n",
      "500 2025-09-12  230.350006  230.789993  226.289993  228.149994  38447500   \n",
      "501 2025-09-15  230.740005  233.729996  230.320007  231.429993  32877127   \n",
      "\n",
      "    Ticker  avg_sentiment_score  avg_sentiment_numeric  article_count  \n",
      "482   AMZN                  NaN                    NaN            NaN  \n",
      "483   AMZN                  NaN                    NaN            NaN  \n",
      "484   AMZN                  NaN                    NaN            NaN  \n",
      "485   AMZN                  NaN                    NaN            NaN  \n",
      "486   AMZN                  NaN                    NaN            NaN  \n",
      "487   AMZN                  NaN                    NaN            NaN  \n",
      "488   AMZN                  NaN                    NaN            NaN  \n",
      "489   AMZN                  NaN                    NaN            NaN  \n",
      "490   AMZN                  NaN                    NaN            NaN  \n",
      "491   AMZN                  NaN                    NaN            NaN  \n",
      "492   AMZN                  NaN                    NaN            NaN  \n",
      "493   AMZN                  NaN                    NaN            NaN  \n",
      "494   AMZN                  NaN                    NaN            NaN  \n",
      "495   AMZN                  NaN                    NaN            NaN  \n",
      "496   AMZN                  NaN                    NaN            NaN  \n",
      "497   AMZN                  NaN                    NaN            NaN  \n",
      "498   AMZN                  NaN                    NaN            NaN  \n",
      "499   AMZN                  NaN                    NaN            NaN  \n",
      "500   AMZN                  NaN                    NaN            NaN  \n",
      "501   AMZN                  NaN                    NaN            NaN  \n"
     ]
    }
   ],
   "source": [
    "print(\"Test_df before FE:\", test_df.shape)\n",
    "print(\"Nulls in sentiment:\", test_df['avg_sentiment_score'].isna().sum())\n",
    "print(test_df.tail(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6febb62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¥ Loading trained models...\n",
      "âœ… Models loaded successfully!\n",
      "ðŸ“Š Dataset shape: (2510, 10)\n",
      "ðŸ“… Date range: 2020-09-16 00:00:00 â†’ 2025-09-15 00:00:00\n",
      "\n",
      "ðŸ§ª TEST WINDOW:\n",
      "Start: 2023-09-01 00:00:00\n",
      "End:   2023-12-15 00:00:00\n",
      "Rows: 148\n",
      "\n",
      "Missing sentiment in test window: 4\n",
      "\n",
      "âœ¨ After FE:\n",
      "Table shape: (116, 31)\n",
      "Feature count: 20\n",
      "ðŸŽ¯ Predictions complete!\n",
      "\n",
      "ðŸ’¾ Saved predictions to: ../data/testing_predictions_clean.csv\n",
      "ðŸŽ‰ Streamlit dashboard can now use EXACT matching dates (2023 only!)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_23260\\3608975919.py:96: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df[\"return_sent_corr\"] = df.groupby(\"Ticker\").apply(\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 09_inference_pipeline (FIXED)\n",
    "# Predict only on VALID sentiment windows\n",
    "# Test window: 2023-09-01 â†’ 2023-12-15\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "print(\"ðŸ”¥ Loading trained models...\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Load artifacts\n",
    "# ------------------------------------------------------------\n",
    "scaler     = joblib.load(\"../models/tab_scaler.pkl\")\n",
    "xgb_model  = joblib.load(\"../models/xgb_final.json\")\n",
    "lgb_model  = joblib.load(\"../models/lgb_final.txt\")\n",
    "meta_model = joblib.load(\"../models/ensemble_final.pkl\")\n",
    "\n",
    "print(\"âœ… Models loaded successfully!\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1) Load merged dataset (same used in training)\n",
    "# ============================================================\n",
    "\n",
    "df = pd.read_csv(\"../data/processed/stocks_news_merged.csv\")\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "\n",
    "df = df.sort_values([\"Ticker\", \"Date\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"ðŸ“Š Dataset shape:\", df.shape)\n",
    "print(\"ðŸ“… Date range:\", df[\"Date\"].min(), \"â†’\", df[\"Date\"].max())\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) FIXED TEST WINDOW â€” sentiment exists here\n",
    "# ============================================================\n",
    "\n",
    "TEST_START = \"2023-09-01\"\n",
    "TEST_END   = \"2023-12-15\"\n",
    "\n",
    "test_df = df[df[\"Date\"].between(TEST_START, TEST_END)].copy()\n",
    "test_df = test_df.sort_values([\"Ticker\", \"Date\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"\\nðŸ§ª TEST WINDOW:\")\n",
    "print(\"Start:\", test_df[\"Date\"].min())\n",
    "print(\"End:  \", test_df[\"Date\"].max())\n",
    "print(\"Rows:\", len(test_df))\n",
    "\n",
    "# Check sentiment availability\n",
    "print(\"\\nMissing sentiment in test window:\", test_df[\"avg_sentiment_score\"].isna().sum())\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) Feature Engineering (same as training)\n",
    "# ============================================================\n",
    "\n",
    "def engineer_features(df):\n",
    "    df = df.copy()\n",
    "    df = df.sort_values([\"Ticker\", \"Date\"]).reset_index(drop=True)\n",
    "\n",
    "    # --- Base Returns ---\n",
    "    df[\"Return\"] = df.groupby(\"Ticker\")[\"Close\"].pct_change()\n",
    "\n",
    "    df[\"Return_lag1\"] = df.groupby(\"Ticker\")[\"Return\"].shift(1)\n",
    "    df[\"Return_lag2\"] = df.groupby(\"Ticker\")[\"Return\"].shift(2)\n",
    "    df[\"Return_lag3\"] = df.groupby(\"Ticker\")[\"Return\"].shift(3)\n",
    "\n",
    "    # --- Rolling ---\n",
    "    df[\"return_ma5\"] = df.groupby(\"Ticker\")[\"Return_lag1\"].transform(lambda x: x.rolling(5).mean())\n",
    "    df[\"Volatility\"] = df.groupby(\"Ticker\")[\"Return_lag1\"].transform(lambda x: x.rolling(5).std())\n",
    "    df[\"Volatility_10\"] = df.groupby(\"Ticker\")[\"Return_lag1\"].transform(lambda x: x.rolling(10).std())\n",
    "\n",
    "    df[\"price_mom5\"] = df.groupby(\"Ticker\")[\"Close\"].pct_change(5)\n",
    "    df[\"price_trend5\"] = df.groupby(\"Ticker\")[\"Close\"].transform(lambda x: x.rolling(5).mean())\n",
    "\n",
    "    # --- Sentiment ---\n",
    "    df[\"sentiment_lag1\"] = df.groupby(\"Ticker\")[\"avg_sentiment_score\"].shift(1)\n",
    "    df[\"sentiment_lag2\"] = df.groupby(\"Ticker\")[\"avg_sentiment_score\"].shift(2)\n",
    "    df[\"sentiment_lag3\"] = df.groupby(\"Ticker\")[\"avg_sentiment_score\"].shift(3)\n",
    "\n",
    "    df[\"sentiment_ma3\"] = df.groupby(\"Ticker\")[\"avg_sentiment_score\"].transform(lambda x: x.rolling(3).mean())\n",
    "    df[\"sentiment_ma5\"] = df.groupby(\"Ticker\")[\"avg_sentiment_score\"].transform(lambda x: x.rolling(5).mean())\n",
    "    df[\"sentiment_std5\"] = df.groupby(\"Ticker\")[\"avg_sentiment_score\"].transform(lambda x: x.rolling(5).std())\n",
    "\n",
    "    df[\"sentiment_mom\"]  = df.groupby(\"Ticker\")[\"avg_sentiment_score\"].diff(1)\n",
    "    df[\"sentiment_mom2\"] = df.groupby(\"Ticker\")[\"avg_sentiment_score\"].diff(2)\n",
    "\n",
    "    df[\"sentiment_vol_interact\"] = df[\"avg_sentiment_score\"] * df[\"Volatility\"]\n",
    "    df[\"sentiment_return_interact\"] = df[\"avg_sentiment_score\"] * df[\"Return_lag1\"]\n",
    "\n",
    "    df[\"return_sent_corr\"] = df.groupby(\"Ticker\").apply(\n",
    "        lambda g: g[\"Return_lag1\"].rolling(5).corr(g[\"avg_sentiment_score\"])\n",
    "    ).reset_index(level=0, drop=True)\n",
    "\n",
    "    # --- RSI ---\n",
    "    def calc_rsi(series, window=10):\n",
    "        delta = series.diff()\n",
    "        gain = delta.clip(lower=0).rolling(window).mean()\n",
    "        loss = -delta.clip(upper=0).rolling(window).mean()\n",
    "        rs = gain / (loss + 1e-9)\n",
    "        return 100 - (100 / (1 + rs))\n",
    "\n",
    "    df[\"RSI_10\"] = df.groupby(\"Ticker\")[\"Close\"].transform(calc_rsi)\n",
    "\n",
    "    # Feature list (same as training)\n",
    "    FEATURES = [\n",
    "        \"Return_lag1\",\"Return_lag2\",\"Return_lag3\",\"return_ma5\",\"Volatility\",\"Volatility_10\",\n",
    "        \"price_mom5\",\"price_trend5\",\n",
    "        \"sentiment_lag1\",\"sentiment_lag2\",\"sentiment_lag3\",\n",
    "        \"sentiment_ma3\",\"sentiment_ma5\",\"sentiment_std5\",\n",
    "        \"sentiment_mom\",\"sentiment_mom2\",\n",
    "        \"sentiment_return_interact\",\"sentiment_vol_interact\",\"return_sent_corr\",\n",
    "        \"RSI_10\"\n",
    "    ]\n",
    "\n",
    "    # Remove rows that lack required rolling-window history\n",
    "    df = df.dropna(subset=FEATURES).reset_index(drop=True)\n",
    "\n",
    "    return df, FEATURES\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Apply FE to test window\n",
    "# ------------------------------------------------------------\n",
    "test_fe, FEATURES = engineer_features(test_df)\n",
    "\n",
    "print(\"\\nâœ¨ After FE:\")\n",
    "print(\"Table shape:\", test_fe.shape)\n",
    "print(\"Feature count:\", len(FEATURES))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4) Make Predictions\n",
    "# ============================================================\n",
    "\n",
    "X = test_fe[FEATURES].values\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "xgb_prob = xgb_model.predict_proba(X_scaled)[:, 1]\n",
    "lgb_prob = lgb_model.predict_proba(X_scaled)[:, 1]\n",
    "\n",
    "meta_input = np.column_stack([xgb_prob, lgb_prob])\n",
    "ensemble_prob = meta_model.predict_proba(meta_input)[:, 1]\n",
    "\n",
    "test_fe[\"Ensemble_Prob\"] = ensemble_prob\n",
    "test_fe[\"Prediction\"]    = (ensemble_prob > 0.5).astype(int)\n",
    "\n",
    "print(\"ðŸŽ¯ Predictions complete!\")\n",
    "test_fe.head()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5) Save Output for Streamlit Dashboard\n",
    "# ============================================================\n",
    "\n",
    "output_path = \"../data/testing_predictions_clean.csv\"\n",
    "test_fe.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"\\nðŸ’¾ Saved predictions to:\", output_path)\n",
    "print(\"ðŸŽ‰ Streamlit dashboard can now use EXACT matching dates (2023 only!)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
