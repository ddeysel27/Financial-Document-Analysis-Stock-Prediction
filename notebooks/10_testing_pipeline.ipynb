{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c0ca9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¥ Testing notebook initialized.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 10_testing_pipeline.ipynb\n",
    "# Testing & Validation Notebook for Final Ensemble Model\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, accuracy_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "\n",
    "print(\"ðŸ”¥ Testing notebook initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a3c7e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Models loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Load trained models\n",
    "# ============================================================\n",
    "\n",
    "scaler = joblib.load(\"../models/tab_scaler.pkl\")\n",
    "xgb_model = joblib.load(\"../models/xgb_final.json\")\n",
    "lgb_model = joblib.load(\"../models/lgb_final.txt\")\n",
    "meta_model = joblib.load(\"../models/ensemble_final.pkl\")\n",
    "\n",
    "print(\"âœ… Models loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c00bfe23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df):\n",
    "    df = df.copy()\n",
    "    df = df.sort_values([\"Ticker\", \"Date\"]).reset_index(drop=True)\n",
    "\n",
    "    df[\"Return\"] = df.groupby(\"Ticker\")[\"Close\"].pct_change()\n",
    "    df[\"Return_lag1\"] = df.groupby(\"Ticker\")[\"Return\"].shift(1)\n",
    "    df[\"Return_lag2\"] = df.groupby(\"Ticker\")[\"Return\"].shift(2)\n",
    "    df[\"Return_lag3\"] = df.groupby(\"Ticker\")[\"Return\"].shift(3)\n",
    "\n",
    "    df[\"return_ma5\"] = df.groupby(\"Ticker\")[\"Return_lag1\"].transform(lambda x: x.rolling(5).mean())\n",
    "    df[\"Volatility\"] = df.groupby(\"Ticker\")[\"Return_lag1\"].transform(lambda x: x.rolling(5).std())\n",
    "    df[\"Volatility_10\"] = df.groupby(\"Ticker\")[\"Return_lag1\"].transform(lambda x: x.rolling(10).std())\n",
    "\n",
    "    df[\"price_mom5\"] = df.groupby(\"Ticker\")[\"Close\"].pct_change(5)\n",
    "    df[\"price_trend5\"] = df.groupby(\"Ticker\")[\"Close\"].transform(lambda x: x.rolling(5).mean())\n",
    "\n",
    "    df[\"sentiment_lag1\"] = df.groupby(\"Ticker\")[\"avg_sentiment_score\"].shift(1)\n",
    "    df[\"sentiment_lag2\"] = df.groupby(\"Ticker\")[\"avg_sentiment_score\"].shift(2)\n",
    "    df[\"sentiment_lag3\"] = df.groupby(\"Ticker\")[\"avg_sentiment_score\"].shift(3)\n",
    "\n",
    "    df[\"sentiment_ma3\"] = df.groupby(\"Ticker\")[\"avg_sentiment_score\"].transform(lambda x: x.rolling(3).mean())\n",
    "    df[\"sentiment_ma5\"] = df.groupby(\"Ticker\")[\"avg_sentiment_score\"].transform(lambda x: x.rolling(5).mean())\n",
    "    df[\"sentiment_std5\"] = df.groupby(\"Ticker\")[\"avg_sentiment_score\"].transform(lambda x: x.rolling(5).std())\n",
    "    df[\"sentiment_mom\"] = df.groupby(\"Ticker\")[\"avg_sentiment_score\"].diff(1)\n",
    "    df[\"sentiment_mom2\"] = df.groupby(\"Ticker\")[\"avg_sentiment_score\"].diff(2)\n",
    "\n",
    "    df[\"sentiment_vol_interact\"] = df[\"avg_sentiment_score\"] * df[\"Volatility\"]\n",
    "    df[\"sentiment_return_interact\"] = df[\"avg_sentiment_score\"] * df[\"Return_lag1\"]\n",
    "\n",
    "    df[\"return_sent_corr\"] = df.groupby(\"Ticker\").apply(\n",
    "        lambda g: g[\"Return_lag1\"].rolling(5).corr(g[\"avg_sentiment_score\"])\n",
    "    ).reset_index(level=0, drop=True)\n",
    "\n",
    "    def calc_rsi(series, window=10):\n",
    "        delta = series.diff()\n",
    "        gain = delta.clip(lower=0).rolling(window).mean()\n",
    "        loss = -delta.clip(upper=0).rolling(window).mean()\n",
    "        rs = gain / (loss + 1e-9)\n",
    "        return 100 - (100 / (1 + rs))\n",
    "\n",
    "    df[\"RSI_10\"] = df.groupby(\"Ticker\")[\"Close\"].transform(calc_rsi)\n",
    "\n",
    "    FEATURES = [\n",
    "        \"Return_lag1\",\"Return_lag2\",\"Return_lag3\",\n",
    "        \"return_ma5\",\"Volatility\",\"Volatility_10\",\n",
    "        \"price_mom5\",\"price_trend5\",\n",
    "        \"sentiment_lag1\",\"sentiment_lag2\",\"sentiment_lag3\",\n",
    "        \"sentiment_ma3\",\"sentiment_ma5\",\"sentiment_std5\",\n",
    "        \"sentiment_mom\",\"sentiment_mom2\",\n",
    "        \"sentiment_return_interact\",\"sentiment_vol_interact\",\"return_sent_corr\",\n",
    "        \"RSI_10\"\n",
    "    ]\n",
    "\n",
    "    # ðŸ”¥ FIXED: Only drop NaNs for feature columns\n",
    "    df = df.dropna(subset=FEATURES).reset_index(drop=True)\n",
    "\n",
    "    return df, FEATURES\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2440ff15",
   "metadata": {},
   "source": [
    "### Load a test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99108e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset shape: (502, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>avg_sentiment_score</th>\n",
       "      <th>avg_sentiment_numeric</th>\n",
       "      <th>article_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>2024-09-13</td>\n",
       "      <td>222.544283</td>\n",
       "      <td>223.002143</td>\n",
       "      <td>220.882021</td>\n",
       "      <td>221.469284</td>\n",
       "      <td>36766600</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>2024-09-16</td>\n",
       "      <td>215.536884</td>\n",
       "      <td>216.213742</td>\n",
       "      <td>212.929026</td>\n",
       "      <td>215.317917</td>\n",
       "      <td>59357400</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>2024-09-17</td>\n",
       "      <td>214.750545</td>\n",
       "      <td>215.895212</td>\n",
       "      <td>213.506336</td>\n",
       "      <td>215.785721</td>\n",
       "      <td>45519300</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>2024-09-18</td>\n",
       "      <td>216.542210</td>\n",
       "      <td>221.678310</td>\n",
       "      <td>216.532247</td>\n",
       "      <td>219.667664</td>\n",
       "      <td>59894900</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>2024-09-19</td>\n",
       "      <td>223.947738</td>\n",
       "      <td>228.755365</td>\n",
       "      <td>223.589405</td>\n",
       "      <td>227.809753</td>\n",
       "      <td>66781300</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date        Open        High         Low       Close    Volume  \\\n",
       "1004 2024-09-13  222.544283  223.002143  220.882021  221.469284  36766600   \n",
       "1005 2024-09-16  215.536884  216.213742  212.929026  215.317917  59357400   \n",
       "1006 2024-09-17  214.750545  215.895212  213.506336  215.785721  45519300   \n",
       "1007 2024-09-18  216.542210  221.678310  216.532247  219.667664  59894900   \n",
       "1008 2024-09-19  223.947738  228.755365  223.589405  227.809753  66781300   \n",
       "\n",
       "     Ticker  avg_sentiment_score  avg_sentiment_numeric  article_count  \n",
       "1004   AAPL                  NaN                    NaN            NaN  \n",
       "1005   AAPL                  NaN                    NaN            NaN  \n",
       "1006   AAPL                  NaN                    NaN            NaN  \n",
       "1007   AAPL                  NaN                    NaN            NaN  \n",
       "1008   AAPL                  NaN                    NaN            NaN  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Load NEW dataset for testing\n",
    "# ============================================================\n",
    "\n",
    "# Example: use last 20% of original dataset OR a new file\n",
    "df_test = pd.read_csv(\"../data/processed/stocks_news_merged.csv\")\n",
    "df_test[\"Date\"] = pd.to_datetime(df_test[\"Date\"])\n",
    "\n",
    "# Only test on unseen data â†’ last 20%\n",
    "cutoff = df_test[\"Date\"].quantile(0.80)\n",
    "df_test = df_test[df_test[\"Date\"] > cutoff].copy()\n",
    "\n",
    "print(\"Test dataset shape:\", df_test.shape)\n",
    "df_test.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b986695",
   "metadata": {},
   "source": [
    "### Generate Ensemble Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34582c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_13632\\1371825813.py:30: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df[\"return_sent_corr\"] = df.groupby(\"Ticker\").apply(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 20)) while a minimum of 1 is required by StandardScaler.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m df_fe, FEATURES = engineer_features(df_test)\n\u001b[32m      7\u001b[39m X = df_fe[FEATURES].values\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m X_scaled = \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m xgb_prob = xgb_model.predict_proba(X_scaled)[:, \u001b[32m1\u001b[39m]\n\u001b[32m     11\u001b[39m lgb_prob = lgb_model.predict_proba(X_scaled)[:, \u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\Data Science\\Fall 2025\\Generative AI\\Project\\genai-financial-doc-analysis\\.venv\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\Data Science\\Fall 2025\\Generative AI\\Project\\genai-financial-doc-analysis\\.venv\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:1045\u001b[39m, in \u001b[36mStandardScaler.transform\u001b[39m\u001b[34m(self, X, copy)\u001b[39m\n\u001b[32m   1042\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m   1044\u001b[39m copy = copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy\n\u001b[32m-> \u001b[39m\u001b[32m1045\u001b[39m X = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1046\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1047\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1048\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1049\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1050\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1051\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1052\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1053\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1055\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sparse.issparse(X):\n\u001b[32m   1056\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.with_mean:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\Data Science\\Fall 2025\\Generative AI\\Project\\genai-financial-doc-analysis\\.venv\\Lib\\site-packages\\sklearn\\base.py:633\u001b[39m, in \u001b[36mBaseEstimator._validate_data\u001b[39m\u001b[34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[39m\n\u001b[32m    631\u001b[39m         out = X, y\n\u001b[32m    632\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[32m--> \u001b[39m\u001b[32m633\u001b[39m     out = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    634\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[32m    635\u001b[39m     out = _check_y(y, **check_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\Data Science\\Fall 2025\\Generative AI\\Project\\genai-financial-doc-analysis\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:1087\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1085\u001b[39m     n_samples = _num_samples(array)\n\u001b[32m   1086\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_samples < ensure_min_samples:\n\u001b[32m-> \u001b[39m\u001b[32m1087\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1088\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m) while a\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1089\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1090\u001b[39m             % (n_samples, array.shape, ensure_min_samples, context)\n\u001b[32m   1091\u001b[39m         )\n\u001b[32m   1093\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array.ndim == \u001b[32m2\u001b[39m:\n\u001b[32m   1094\u001b[39m     n_features = array.shape[\u001b[32m1\u001b[39m]\n",
      "\u001b[31mValueError\u001b[39m: Found array with 0 sample(s) (shape=(0, 20)) while a minimum of 1 is required by StandardScaler."
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Generate Predictions from Ensemble\n",
    "# ============================================================\n",
    "\n",
    "df_fe, FEATURES = engineer_features(df_test)\n",
    "\n",
    "X = df_fe[FEATURES].values\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "xgb_prob = xgb_model.predict_proba(X_scaled)[:, 1]\n",
    "lgb_prob = lgb_model.predict_proba(X_scaled)[:, 1]\n",
    "\n",
    "meta_input = np.column_stack([xgb_prob, lgb_prob])\n",
    "ensemble_prob = meta_model.predict_proba(meta_input)[:, 1]\n",
    "\n",
    "df_fe[\"Pred_Prob\"] = ensemble_prob\n",
    "df_fe[\"Prediction\"] = (ensemble_prob > 0.5).astype(int)\n",
    "\n",
    "df_fe.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
